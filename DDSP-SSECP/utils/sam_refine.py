"""
SAM-S2CPL: SAM-driven Structure-Semantic Collaborative Pseudo-Labeling

This module provides utilities to integrate SAM-Med3D into pseudo-label refinement,
using the SAM model as a "structural expert" to refine boundaries generated by the
teacher model's "semantic" predictions.

Key Components:
1. generate_prompts_from_mask: Extract center points from high-confidence mask regions
2. sam_refine_pseudo_label: Refine pseudo-labels using SAM-Med3D predictions
"""

import torch
import torch.nn.functional as F
import numpy as np
from scipy import ndimage


def generate_prompts_from_mask(mask, num_points=1, foreground=True):
    """
    Generate point prompts from a binary mask by extracting geometric centers
    of connected components.
    
    Args:
        mask: Binary mask tensor (D, H, W) or (B, D, H, W)
        num_points: Maximum number of center points to extract per class
        foreground: If True, generate positive points; otherwise negative points
        
    Returns:
        points_coords: Tensor of point coordinates (1, N, 3) in (x, y, z) format
        points_labels: Tensor of point labels (1, N) where 1=foreground, 0=background
    """
    if mask.dim() == 4:
        mask = mask[0]  # Remove batch dimension
    
    mask_np = mask.cpu().numpy().astype(np.uint8)
    
    # Handle empty masks
    if mask_np.sum() == 0:
        return None, None
    
    # Label connected components
    labeled_array, num_features = ndimage.label(mask_np)
    
    points = []
    for i in range(1, min(num_features + 1, num_points + 1)):
        # Get center of mass for each connected component
        component_mask = (labeled_array == i)
        if component_mask.sum() > 0:
            center = ndimage.center_of_mass(component_mask)
            # center is (z, y, x) but SAM expects (x, y, z)
            points.append([center[2], center[1], center[0]])
    
    if len(points) == 0:
        # Fallback to single center point
        center = ndimage.center_of_mass(mask_np)
        points.append([center[2], center[1], center[0]])
    
    points_coords = torch.tensor(points, dtype=torch.float32).unsqueeze(0)  # (1, N, 3)
    points_labels = torch.ones(1, len(points), dtype=torch.long) if foreground else torch.zeros(1, len(points), dtype=torch.long)
    
    return points_coords, points_labels


def pad_to_128(image):
    """
    Pad image to 128x128x128 as required by SAM-Med3D.
    
    Args:
        image: Input tensor (B, C, D, H, W)
        
    Returns:
        padded_image: Padded tensor (B, C, 128, 128, 128)
        pad_info: Dictionary containing original shape and padding offsets
    """
    _, _, d, h, w = image.shape
    target_size = 128
    
    # Calculate padding (centered)
    pad_d = max(0, target_size - d)
    pad_h = max(0, target_size - h)
    pad_w = max(0, target_size - w)
    
    pad_d_front = pad_d // 2
    pad_d_back = pad_d - pad_d_front
    pad_h_front = pad_h // 2
    pad_h_back = pad_h - pad_h_front
    pad_w_front = pad_w // 2
    pad_w_back = pad_w - pad_w_front
    
    # F.pad order: (w_left, w_right, h_left, h_right, d_left, d_right)
    padded_image = F.pad(image, (pad_w_front, pad_w_back, 
                                   pad_h_front, pad_h_back, 
                                   pad_d_front, pad_d_back), mode='constant', value=0)
    
    pad_info = {
        'original_shape': (d, h, w),
        'pad_offsets': (pad_d_front, pad_h_front, pad_w_front),
        'target_size': target_size
    }
    
    return padded_image, pad_info


def unpad_from_128(mask, pad_info):
    """
    Remove padding from SAM output to restore original size.
    
    Args:
        mask: Padded mask tensor (B, C, 128, 128, 128) or (D, H, W)
        pad_info: Dictionary containing original shape and padding offsets
        
    Returns:
        unpadded_mask: Tensor with original spatial dimensions
    """
    d_orig, h_orig, w_orig = pad_info['original_shape']
    d_offset, h_offset, w_offset = pad_info['pad_offsets']
    
    if mask.dim() == 5:
        # (B, C, D, H, W)
        unpadded = mask[:, :, d_offset:d_offset+d_orig, 
                        h_offset:h_offset+h_orig, 
                        w_offset:w_offset+w_orig]
    elif mask.dim() == 3:
        # (D, H, W)
        unpadded = mask[d_offset:d_offset+d_orig, 
                       h_offset:h_offset+h_orig, 
                       w_offset:w_offset+w_orig]
    else:
        raise ValueError(f"Unexpected mask dimension: {mask.dim()}")
    
    return unpadded


def scale_points_for_sam(points_coords, original_shape, target_shape=(128, 128, 128)):
    """
    Scale point coordinates from original image space to SAM input space (128x128x128).
    
    Args:
        points_coords: Point coordinates tensor (1, N, 3) in (x, y, z) format
        original_shape: Original image shape (D, H, W)
        target_shape: SAM target shape (128, 128, 128)
        
    Returns:
        scaled_coords: Scaled point coordinates
    """
    if points_coords is None:
        return None
    
    d_orig, h_orig, w_orig = original_shape
    d_tar, h_tar, w_tar = target_shape
    
    # Calculate padding offsets (centered padding)
    pad_d = max(0, d_tar - d_orig) // 2
    pad_h = max(0, h_tar - h_orig) // 2
    pad_w = max(0, w_tar - w_orig) // 2
    
    # Scale factors (if original is larger than 128, we scale down; otherwise we just add offset)
    scale_x = min(1.0, w_tar / w_orig)
    scale_y = min(1.0, h_tar / h_orig)
    scale_z = min(1.0, d_tar / d_orig)
    
    scaled_coords = points_coords.clone()
    # points_coords is (x, y, z)
    scaled_coords[..., 0] = points_coords[..., 0] * scale_x + pad_w
    scaled_coords[..., 1] = points_coords[..., 1] * scale_y + pad_h
    scaled_coords[..., 2] = points_coords[..., 2] * scale_z + pad_d
    
    return scaled_coords


def sam_refine_mask(sam_model, image, seed_mask, original_shape, device, num_clicks=1):
    """
    Use SAM-Med3D to refine a seed mask by generating prompts and running inference.
    
    Args:
        sam_model: Loaded SAM-Med3D model (frozen parameters)
        image: Input image tensor (B, C, D, H, W)
        seed_mask: High-confidence seed mask (D, H, W) binary tensor
        original_shape: Original image shape before any SAM-specific padding
        device: CUDA device
        num_clicks: Number of click iterations for SAM
        
    Returns:
        refined_mask: Binary mask refined by SAM (D, H, W)
    """
    if seed_mask.sum() == 0:
        return seed_mask  # Return empty mask if no seeds
    
    # Generate prompts from seed mask
    points_coords, points_labels = generate_prompts_from_mask(seed_mask, num_points=1, foreground=True)
    
    if points_coords is None:
        return seed_mask
    
    # Pad image to 128x128x128
    padded_image, pad_info = pad_to_128(image)
    padded_image = padded_image.to(device)
    
    # Scale points to padded space
    scaled_points = scale_points_for_sam(points_coords, original_shape, (128, 128, 128))
    if scaled_points is not None:
        scaled_points = scaled_points.to(device)
        points_labels = points_labels.to(device)
    
    with torch.no_grad():
        # Image encoding (heavy computation)
        image_embeddings = sam_model.image_encoder(padded_image)
        
        # Initialize low-res mask
        prev_low_res_mask = torch.zeros(1, 1, 8, 8, 8, device=device, dtype=torch.float)
        
        all_points_coords = torch.zeros(1, 0, 3, device=device)
        all_points_labels = torch.zeros(1, 0, device=device)
        
        for _ in range(num_clicks):
            # Accumulate points
            all_points_coords = torch.cat([all_points_coords, scaled_points], dim=1)
            all_points_labels = torch.cat([all_points_labels, points_labels.float()], dim=1)
            
            # Prompt encoding
            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(
                points=[all_points_coords, all_points_labels.long()],
                boxes=None,
                masks=prev_low_res_mask,
            )
            
            # Mask decoding
            low_res_masks, _ = sam_model.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=sam_model.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
            )
            
            prev_low_res_mask = low_res_masks.detach()
        
        # Upsample to full resolution
        final_masks_hr = F.interpolate(
            low_res_masks,
            size=(128, 128, 128),
            mode='trilinear',
            align_corners=False
        )
        
        # Apply sigmoid and threshold
        sam_mask = torch.sigmoid(final_masks_hr) > 0.5
        sam_mask = sam_mask.squeeze()  # (128, 128, 128)
        
        # Unpad to original size
        refined_mask = unpad_from_128(sam_mask, pad_info)
    
    return refined_mask.float()


def sam_refine_pseudo_labels(sam_model, image, pseudo_label, confidence_mask, n_classes, device, num_clicks=1):
    """
    Refine pseudo-labels using SAM-Med3D for each foreground class.
    
    SAM acts as a "structural expert" to improve boundary quality of pseudo-labels
    generated by the teacher model.
    
    Args:
        sam_model: Loaded SAM-Med3D model (frozen)
        image: Target domain image (B, C, D, H, W)
        pseudo_label: Initial pseudo-label indices (D, H, W) 
        confidence_mask: High-confidence mask from teacher model (D, H, W)
        n_classes: Number of classes
        device: CUDA device
        num_clicks: Number of SAM click iterations
        
    Returns:
        refined_pseudo_label: Refined pseudo-labels (D, H, W)
    """
    original_shape = image.shape[2:]  # (D, H, W)
    refined_pseudo_label = pseudo_label.clone()
    
    # Process each foreground class
    for cls_idx in range(1, n_classes):  # Skip background (class 0)
        # Get seed mask: high-confidence regions for this class
        class_mask = (pseudo_label == cls_idx).float()
        seed_mask = class_mask * confidence_mask
        
        if seed_mask.sum() == 0:
            continue  # No seeds for this class
        
        # SAM refinement for this class
        refined_class_mask = sam_refine_mask(
            sam_model, image, seed_mask, original_shape, device, num_clicks
        )
        
        # Fuse SAM output with original class assignment
        # SAM provides better boundaries; we assign the class label to SAM-refined regions
        # Only update regions where SAM expanded the mask (preserves original high-confidence seeds)
        new_class_regions = (refined_class_mask > 0) & (pseudo_label != cls_idx)
        
        # Assign class to new regions found by SAM
        refined_pseudo_label[new_class_regions] = cls_idx
    
    return refined_pseudo_label


def initialize_sam_model(checkpoint_path, device, model_type='vit_b_ori'):
    """
    Initialize SAM-Med3D model with frozen parameters.
    
    Args:
        checkpoint_path: Path to SAM-Med3D checkpoint (sam_med3d_turbo.pth)
        device: CUDA device
        model_type: Model variant ('vit_b_ori' for 128x128x128 input)
        
    Returns:
        sam_model: Loaded and frozen SAM-Med3D model
    """
    from segment_anything.build_sam3D import sam_model_registry3D
    
    sam_model = sam_model_registry3D[model_type](checkpoint=checkpoint_path)
    sam_model = sam_model.to(device)
    sam_model.eval()
    
    # Freeze all parameters
    for param in sam_model.parameters():
        param.requires_grad = False
    
    return sam_model
